{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a random waveform and spectrogram\n",
    "audioFile = 'data/genres_original/blues/blues.00000.wav'\n",
    "waveform, sampleRate = librosa.load(audioFile)\n",
    "print('Class : Blues\\n')\n",
    "ipd.display(ipd.Audio(waveform, rate = sampleRate))\n",
    "\n",
    "\n",
    "# Displaying waveform\n",
    "plt.figure(figsize = (15, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(waveform)\n",
    "plt.title('Waveform', fontsize = 16)\n",
    "plt.xlabel('Sample Index', fontsize = 12)\n",
    "plt.ylabel('Amplitude', fontsize = 12)\n",
    "\n",
    "\n",
    "# Displaying spectrogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(waveform, Fs = sampleRate)\n",
    "plt.title('Spectrogram', fontsize = 16)\n",
    "plt.xlabel('Time (s)', fontsize = 12)\n",
    "plt.ylabel('Frequency (Hz)', fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the duration of the audio file\n",
    "# Waveform is a 1D numpy array containing the audio signal\n",
    "print(f'Waveform shape : {waveform.shape} -> 1D numpy array')\n",
    "# the resulting shapeis the sample Hz times the duration of the audio file\n",
    "print(f'Audio duration : {(waveform.shape[0]/sampleRate).__round__(2)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to ensure that all audio examples in the dataset have the same sampling rate when working on any audio task. And the sampling rate of the data should match the sampling rate of the data the model was pre-trained on if not we need to do the resampling process.\n",
    "\n",
    "The sampling rate determines the time interval between successive audio samples, which impacts the temporal resolution of the audio data. For example, a 5-second sound at a sampling rate of 16,000 Hz will be represented as a series of 80,000 values. (Hz*len(audio) in seconds) \n",
    "\n",
    "Hertz : equals the number of cycles per second (svingninger per sekond)\n",
    "\n",
    "The amplitude of a sound describes the sound pressure level at any given instant and is measured in decibels(dB). The bit depth of the sample determines with how much precision this amplitude value can be described.\n",
    "\n",
    "Amplitude change is represented by y-axis as plotted by time. \"This is also known as the time domain representation of sound.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(waveform, sr=sampleRate)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The frequency spectrum\n",
    "#### discrete Fourier transform or DFT\n",
    "\n",
    "Another way to visualize audio data is to plot the frequency spectrum of an audio signal, also known as the \"frequency domain representation\". The spectrum is computed using the discrete Fourier transform or DFT. It describes the individual frequencies that make up the signal and how strong they are.\n",
    "\n",
    "Here, we are going to plot the frequency spectrum for the using numpy's rfft() function. While it is possible to plot the spectrum of the entire sound, it's more useful to look at a small region instead. Here we will take the DFT over the first 4096 samples, which is roughly the length of the first note being played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the frequency spectrum\n",
    "\n",
    "dft_input=waveform[:4096]\n",
    "\n",
    "# calculate the DFT\n",
    "window =np.hanning(len(dft_input))\n",
    "windowed_input=dft_input*window\n",
    "dft=np.fft.rfft(windowed_input)\n",
    "\n",
    "# get the amplitude spectrum in decibels\n",
    "amplitude=np.abs(dft)\n",
    "amplitude_db=librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency=librosa.fft_frequencies(sr=sampleRate, n_fft=len(dft_input))\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude (dB)')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram (discrete Fourier transform or DFT)\n",
    "The waveform plots the amplitude of the audio signal over time, the spectrum visualizes the amplitudes of the individual frequencies at a fixed point in time. What if we want to see how the frequencies in an audio signal change? The problem is that the spectrum only shows a frozen snapshot of the frequencies at a given instant. The solution is to take multiple DFTs, each covering only a small slice of time, and stack the resulting spectra together into a spectrogram. The algorithm that performs this computation is the STFT of Short Fourier Transform.\n",
    "\n",
    "\n",
    "In this plot, the x-axis represents time as in the waveform visualization but now the y-axis represents frequency in Hz. The intensity of the color gives the amplitude of power of the frequency component at each point in time, measured in decibels(dB). The spectrogram is created by taking shot segments of the audio signal, typically lasting a few milliseconds, and calculating the discrete Fourier transform of each segment to obtain its frequency spectrum. The resulting spectra are then stacked together on the time axis to create the spectrogram. Each vertical slice in this image corresponds to a single frequency spectrum, seen from the top. By default, librosa.stft() splits the audio signal into segments of 2048 samples, which gives a good-trade-off between frequency resolution and time resolution.\n",
    "\n",
    "Since the spectrogram and the waveform are different views of the same data, it's possible to turn the spectrogram back into the original waveform using the inverse STFT. In that case, we can use a phase reconstruction algorithm such as the classic Griffin-Lim algorithm, or using a neural network called a vector, to reconstruct a waveform from the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=librosa.stft(waveform, n_fft=2048, hop_length=250, win_length=1000, window='hann')\n",
    "print(D.shape)\n",
    "S_db=librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_db, x_axis='time', y_axis='hz')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel spectrogram\n",
    "A mel spectrogram is a variation of the spectrogram that is commonly used in speech processing and machine learning tasks. It is similar to a spectrogram in that it shoes the frequency content of an audio signal over time, but on a different frequency axis.\n",
    "\n",
    "In the example below, n_mel stands for the number of mel bands to generate. The mel bands define a set of frequency ranges that divide the spectrum into preceptually meaningful components, using a set of filters whose shape and spacing are chosen to mimic the way the human ears responds to different frequencies. Common values for n_mels are 40 or 80 fmax indicates the highest frequency(in Hz) we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=librosa.feature.melspectrogram(y=waveform, sr=sampleRate, n_mels=140, fmax=9500)\n",
    "S_dB=librosa.power_to_db(S,ref=np.max)\n",
    "print(S.shape)\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sampleRate, fmax=9500)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Env variables\n",
    "os.environ['DATASET']='data/genres_original'\n",
    "os.environ['MODEL']='ntu-spml/distilhubert'\n",
    "os.environ[\"WANDB_NAME\"] = \"hubert-10-genres\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Fine-tuning HuBERT\"\n",
    "os.environ[\"WANDB_NOTES\"] = \"Fine-tuning HuBERT on gtzan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Define Path to the dataset\n",
    "dataset=load_dataset(os.getenv('DATASET'))\n",
    "train_data = dataset['train'].train_test_split(seed=42, shuffle=True, stratify_by_column=\"label\",test_size=.4)\n",
    "test_val_data = train_data['test'].train_test_split(seed=42, shuffle=True, stratify_by_column=\"label\",test_size=.5)\n",
    "\n",
    "dataset[\"train\"] = train_data[\"train\"]\n",
    "dataset[\"test\"] = test_val_data[\"test\"]\n",
    "dataset[\"validation\"] = test_val_data[\"train\"]\n",
    "\n",
    "\n",
    "# test_val_data[\"val\"] = test_val_data[\"train\"]\n",
    "# train_data = train_data[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\"*25, \"DATASET\", \"#\"*25)\n",
    "print('\\n')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor=AutoFeatureExtractor.from_pretrained(os.getenv('MODEL'), do_normalize=True, return_attention_mask=True, sampling_rate=22050)\n",
    "\n",
    "sampling_rate=feature_extractor.sampling_rate\n",
    "print(f'DistilHuBERT Sampling Rate: {sampling_rate} Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration=30.0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # extracting and saving arrays\n",
    "    audio_arrays=[x['array'] for x in examples['audio']]\n",
    "    \n",
    "    # preprocessing audio inputs\n",
    "    inputs=feature_extractor(audio_arrays, sampling_rate=sampling_rate, max_length=int(sampling_rate*max_duration), truncation=True, return_attention_mask=True)\n",
    "    return inputs\n",
    "\n",
    "dataset_encoded=dataset.map(preprocess_function, remove_columns=['audio'], batched=True, batch_size=100, num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric=evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions=np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=os.getenv('WANDB_NAME'),\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    learning_rate=5e-5,\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=100, # We control the total training steps to fit the limied resources\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False, # set to true for cuda\n",
    "    save_total_limit=2,\n",
    "    report_to='wandb',\n",
    "    run_name=os.getenv('WANDB_NAME')\n",
    ")\n",
    "\n",
    "trainer=Trainer(model=model, args=training_args, train_dataset=dataset_encoded['train'], eval_dataset=dataset_encoded['validation'],tokenizer=feature_extractor, compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
