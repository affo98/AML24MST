{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset, load_dataset, Audio\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import torchmetrics\n",
    "import torchaudio\n",
    "import wandb\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up genre names and their codes\n",
    "genre_names = [\n",
    "    \"blues\",\n",
    "    \"classical\",\n",
    "    \"country\",\n",
    "    \"disco\",\n",
    "    \"hiphop\",\n",
    "    \"jazz\",\n",
    "    \"metal\",\n",
    "    \"pop\",\n",
    "    \"reggae\",\n",
    "    \"rock\",\n",
    "]\n",
    "genre_codes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Import these into your working script to make sure that we all have the same codes\n",
    "id2label = {id_: label for id_, label in zip(genre_codes, genre_names)}\n",
    "label2id = {label: id_ for label, id_ in zip(genre_names, genre_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATA_BASELINE_TRAIN'] = '/kaggle/input/data-train-val-test/data_train_val_test'\n",
    "os.environ['DATA_BASELINE_NOISY_TRAIN'] = '/kaggle/input/data-noisy-train-val-test/data_noisy_train_val_test'\n",
    "os.environ['DATA_BASELINE_GENERATED_TRAIN'] = '/kaggle/input/aml24mst/data_train_val_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = load_dataset(os.getenv('DATA_BASELINE_TRAIN'))\n",
    "test_set = df_baseline.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise = load_dataset(os.getenv('DATA_BASELINE_NOISY_TRAIN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = load_dataset(os.getenv('DATA_BASELINE_GENERATED_TRAIN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "sampling_rate = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot normal audiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'test_set' is your dataset and 'sampling_rate' is defined\n",
    "test_inputs = []\n",
    "for k in [0, 20, 40, 60, 80, 100, 120, 140, 160, 180]:  # Adjust the range as needed\n",
    "    file = test_set[k]['audio']['array']\n",
    "    test_inputs.append(file)\n",
    "\n",
    "# Plotting the collected test_inputs in subplots\n",
    "num_plots = len(test_inputs)\n",
    "num_cols = 5\n",
    "num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate the number of rows needed\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.plot(test_input)\n",
    "    plt.title(f'{genre_names[i-1]}')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'waveforms.png')\n",
    "FileLink(r'waveforms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'test_set' is your dataset and 'sampling_rate' is defined\n",
    "test_inputs = []\n",
    "for k in [0,20,40,60,80,100,120,140,160,180]:  # Adjust the range as needed\n",
    "    file = test_set[k]['audio']['array']\n",
    "    test_input = feature_extractor(file, sampling_rate=sampling_rate, return_tensors='pt')\n",
    "    test_inputs.append(test_input['input_values'].numpy())\n",
    "\n",
    "# Plotting the collected test_inputs in subplots\n",
    "num_plots = len(test_inputs)\n",
    "num_cols = 5\n",
    "num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate the number of rows needed\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i, test_input in enumerate(test_inputs,1 ):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.imshow(test_input.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.title(f'{genre_names[i-1]}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'spectograms.png')\n",
    "FileLink(r'spectograms.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot normal and noise audiofile example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name_search(file_name):\n",
    "    '''\n",
    "    Find file-name in the path of the file. \n",
    "    '''\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        pattern = r'([^/]+\\.wav)'\n",
    "    else:\n",
    "        pattern = r'(.+\\\\)?(.+\\.wav)'\n",
    "    \n",
    "    match = re.search(pattern, file_name)\n",
    "    return match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = file_name_search(df_noise['train'][120]['audio']['path'])\n",
    "noise_path = file_name_search(df_noise['train'][180]['audio']['path'])\n",
    "print(f\"{clean_path}\")\n",
    "print(f\"{noise_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = df_noise['train'][120]['audio']['array']\n",
    "noise = df_noise['train'][180]['audio']['array']\n",
    "clean_spec = feature_extractor(clean, sampling_rate=sampling_rate, return_tensors='pt')['input_values'].numpy()\n",
    "noise_spec = feature_extractor(noise, sampling_rate=sampling_rate, return_tensors='pt')['input_values'].numpy()\n",
    "\n",
    "blues1 = [clean, noise, clean_spec, noise_spec]\n",
    "\n",
    "# Plotting the collected test_inputs in subplots\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "for i, input in enumerate(blues1, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    if i <3:\n",
    "        plt.plot(input)\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Amplitude')\n",
    "        if i == 1:\n",
    "            plt.title(f'Original data')\n",
    "        else:\n",
    "            plt.title(f'Noise data')\n",
    "    else:\n",
    "        plt.imshow(input.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Frequency')\n",
    "   \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'original_noise.png')\n",
    "FileLink(r'original_noise.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_example = df_gen['train'][201]['audio']['array']\n",
    "gen_example_spec = feature_extractor(gen_example, sampling_rate=sampling_rate, return_tensors='pt')['input_values'].numpy()\n",
    "\n",
    "# Plotting the collected test_inputs in subplots\n",
    "num_cols = 1\n",
    "num_rows = 2\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i, input in enumerate([gen_example, gen_example_spec], 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    if i ==1:\n",
    "        plt.plot(input)\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.title(f'Generated Data')\n",
    "    else:\n",
    "        plt.imshow(input.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Frequency')\n",
    "   \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'generated_example.png')\n",
    "FileLink(r'generated_example.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
